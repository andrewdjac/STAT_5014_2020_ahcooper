---
title: "HW4_ahcooper"
author: "Andrew Cooper"
date: "10/11/2020"
output: pdf_document
---

```{r warning = F, message = F}
library(tidyverse)
library(knitr)
library(microbenchmark)
library(parallel)
```


# Problem 2

```{r}
grad_descent <- function(X, h, theta0, theta1, alpha = 1e-7, tol = 1e-9){
  m <- nrow(X)
  theta0_old <- theta0
  theta1_old <- theta1
  diff0 <- diff1 <- 1
  num_iter <- 0
  while(diff0 > tol && diff1 > tol && num_iter < 5e6){
    h0 <- X %*% c(theta0_old, theta1_old)
    theta0_new <- theta0_old - alpha*(1/m)*sum(h0 - h)
    theta1_new <- theta1_old - alpha*(1/m)*sum((h0 - h)*X[, 2])
    diff0 <- abs(theta0_new - theta0_old)
    diff1 <- abs(theta1_new - theta1_old)
    theta0_old <- theta0_new
    theta1_old <- theta1_new
    num_iter <- num_iter + 1
  }
  return(list(
    "theta" = c(theta0_new, theta1_new),
    "num_iter" = num_iter))
}
```



```{r}
set.seed(1256)
theta <- as.matrix(c(1, 2), nrow = 2)
X <- cbind(1, rep(1:10, 10))
h <- X %*% theta + rnorm(100, 0, 0.2)
grad_descent(X, h, 1, 4, 0.01)
```

```{r}
lm(h ~ 0 + X)
```

The results are about the same.



# Problem 3

## Part a

```{r}
n <- 15
starting_vals <- seq(1 - n, 1 + n)
grid_inds <- expand.grid(seq(1, n, 1), seq(1, n, 1))
```


```{r}
cl <- makeCluster(10)
clusterExport(cl, "grad_descent")
clusterExport(cl, "X")
clusterExport(cl, "h")
clusterExport(cl, "starting_vals")
```


```{r}
starting_value_analysis <- parApply(cl, grid_inds, 1, function(row){
  i <- row[1]
  j <- row[2]
  return(est <- grad_descent(X, h, starting_vals[i], starting_vals[j], 0.01))
})
```


```{r}
beta_estimates <- sapply(1:length(starting_value_analysis), function(i){starting_value_analysis[[i]][[1]]})
iterations <-  sapply(1:length(starting_value_analysis), function(i){starting_value_analysis[[i]][[2]]})
```


The minimum number of iterations was `r min(iterations)` when our starting value for $\theta_0$ was `r grid_inds[which.min(iterations),][1]` and our starting values for $\theta_1$ was `r grid_inds[which.min(iterations),][2]`.

The maximum number of iterations was `r max(iterations)` when our starting value for $\theta_0$ was `r grid_inds[which.max(iterations),][1]` and our starting values for $\theta_1$ was `r grid_inds[which.max(iterations),][2]`.

The mean beta estimate was `r apply(beta_estimates, 1, mean)`. The standard deviation was `r apply(beta_estimates, 1, sd)`.

## Part b

No this would not be a good idea. The purpose of these algorithms is to search for and approximate unknown values. In this case we can actually calculate the exact solution, but in most others cases where the function is not convex we can't find an exact formula for the solution and need to use an algorithm like this to find it.

## Part c

It is nice that it is such a simple approach, however this can be really slow and take too many iterations, especially toward the end. A more advanced/complex algorithm would likely find approximations to the minimum of the objective function with fewer iterations.

# Problem 4

This is the exact solution for the values of $[\theta_0, \theta_2]$ that minimize the sum of squared-errors, $(Y - X\hat{\beta})^T(Y - X\hat{\beta})$. We can derive an equation for the minimum of this objective function since it is quadratic and therefore convex. We derive this solution by taking the derivative of the objective function with respect to $\hat{\beta}$, then equating it to $0$ and solving.

To calculate this matrix we need to invert the square of the data matrix $X$, which we know is invertible due to its symmetry. We can compute the exact inverse using a program like R, although it is an $O(n^3)$ operation which is relatively slow. We could instead calculate a pseudo-inverse, which adds some error to the calculation but is usually significantly quicker.


# Problem 5


```{r}
set.seed(12456)

G <- matrix(sample(c(0, 0.5, 1), size = 16000, replace = T), ncol = 10)
R <- cor(G)
C <- kronecker(R, diag(1600))
id <- sample(1:1600, size = 932, replace = F)
q <- sample(c(0, 0.5, 1), size = 15068, replace = T)
A <- C[id, -id]
B <- C[-id, -id]
p <- runif(932, 0, 1)
r <- runif(15068, 0, 1)
C <- NULL
```


## Part a

A is `r object.size(A)`. B is `r object.size(B)`.

```{r eval = F}
microbenchmark({
  p + A %*% solve(B) %*% t(t(q - r))
}, times = 1)
```


## Part b

We would likely want to break apart the inverse operation from the rest of the 

## Part c

```{r}

```


# Problem 3 (2)

## Part a

```{r}
success_rate <- function(v, s){
  return(length(which(v == s)) / length(v))
}
```


## Part b

```{r}
set.seed(12345)
P4b_data <- matrix(rbinom(10, 1, prob = (31:40)/100), nrow = 10, ncol = 10, byrow = F)
```


## Part c

```{r}
apply(P4b_data, 2, function(x){success_rate(x, 1)})
```

The issue with this is the creation of the matrix "P4b_data". Instead of simulating the results of ten tosses for each of the different probabilities, we only simulated one series of 10 flips and made each column that same series.

## Part d

```{r}
calc_prob <- function(p){
  return(rbinom(10, 1, p))
}
```


```{r}
P4b_data <- sapply((31:40)/100, calc_prob)
```


```{r}
apply(P4b_data, 2, function(x){success_rate(x, 1)})
```


These are the correct estimates of the marginal success rates.


# Problem 4 (2)

```{r}
df <- readRDS("HW3_data.rds")
```

```{r}
df <- df %>% 
  rename(x = dev1,
         y = dev2)
```


```{r}
create_scatter <- function(df, title, label){
  df %>% 
    plot(main = title, xlab = label[1], ylab = label[2])
}
```


```{r}
create_scatter(df[c(2,3)], "dev2 vs. dev1", c("dev1", "dev2"))
```


```{r}
par(mfrow = c(1, 3))
sapply(seq(1, 13, 1), function(i){
  df_filtered <- df %>% filter(Observer == i)
  create_scatter(df_filtered[c(2, 3)], "dev2 vs. dev1", c("dev1", "dev2"))
})
```


# Problem 5 (2)

## Part a

```{r}
library(downloader)
download("http://www.farinspace.com/wp-content/uploads/us_cities_and_states.zip",dest="us_cities_states.zip")
unzip("us_cities_states.zip")

#read in data, looks like sql dump, blah
library(data.table)
states <- fread(input = "./us_cities_and_states/states.sql",skip = 23,sep = "'", sep2 = ",", header = F, select = c(2,4))
### YOU do the CITIES
### I suggest the cities_extended.sql may have everything you need
### can you figure out how to limit this to the 50?
cities <- fread(input = "./us_cities_and_states/cities_extended.sql", sep = "'", sep2 = ",", header = F, select = c(2, 4))
```

## Part b

```{r}
cities %>% 
  rename(State = V4) %>% 
  unique() %>%
  group_by(State) %>% 
  summarise("Number of Cities" = n()) %>% 
  kable()
```


## Part c


```{r}
count_frequency <- function(letter, state_name){
  v <- strsplit(state_name, "") %>% unlist()
  return(sum(tolower(v) == tolower(letter)))
}
```


```{r}
letter_count <- data.frame(matrix(NA, nrow = 50, ncol = 26))
getCount <- function(state){
  return(sapply(letters, function(x){count_frequency(x, state)}))
}
```


```{r}
letter_count <- sapply(states$V2, function(x){getCount(x)}) %>% t()
```


## Part d


```{r}
library(fiftystater)

data("fifty_states") # this line is optional due to lazy data loading
crimes <- data.frame(state = tolower(rownames(USArrests)), USArrests)
# map_id creates the aesthetic mapping to the state name column in your data
p <- ggplot(crimes, aes(map_id = state)) + 
  # map points to the fifty_states shape data
  geom_map(aes(fill = Assault), map = fifty_states) + 
  expand_limits(x = fifty_states$long, y = fifty_states$lat) +
  coord_map() +
  scale_x_continuous(breaks = NULL) + 
  scale_y_continuous(breaks = NULL) +
  labs(x = "", y = "") +
  theme(legend.position = "bottom", 
        panel.background = element_blank())

p
#ggsave(plot = p, file = "HW6_Problem6_Plot_Settlage.pdf")
```



# Problem 2 (3)


## Part a

The issue with the code is in the "lm" statement in the for-loop. The formula uses existing data frames, not the names of variables in df08. The lm function just ignores the data argument in this case, which is why the result is just the same as the original regression. To fix this you need to change the variable names to those in df08.

## Part b

```{r warning = F, message = F}
Sensory_raw <- read_table2("https://www2.isye.gatech.edu/~jeffwu/wuhamadabook/data/Sensory.dat", 
     col_types = cols(Item = col_character()), col_names = F, skip = 2)
```


```{r warning = F, message = F}
# Remove NA's, align columns
Sensory_values <- apply(Sensory_raw, 1, function(x){
  if(any(is.na(x))){
    return(x[!is.na(x)])
  }else{
    return(x[-1])
  }
})
# Add column for item
Sensory <- data.frame(cbind(rep(seq(1, 10, 1), each = 3), t(Sensory_values)))
colnames(Sensory) <- c("Item", seq(1, 5, 1))
Sensory <- as_data_frame(Sensory)
```


```{r}
Sensory <- Sensory %>% 
  gather("Operator", "Y", -Item)
```


```{r}
bootstrap_size <- 30
bootstrap_sd <- matrix(nrow = 100, ncol = 5)
for(i in 1:100){
  bootstrap_sample <- purrr::map(1:5, function(x){
    df_filtered <- Sensory[which(Sensory$Operator == x), ]
    bootstrap_inds <- sample(1:nrow(df_filtered), bootstrap_size, replace = T)
    return(df_filtered[bootstrap_inds, ])
  }) %>% combine()
  bootstrap_sd[i, ] <- lm(Y ~ Operator, bootstrap_sample) %>% coef()
}
```


```{r}
apply(bootstrap_sd, 2, sd)
```

```{r}
bootstrap_nonparallel_results <- microbenchmark({
  bootstrap_size <- 30
  bootstrap_sd <- matrix(nrow = 100, ncol = 5)
  for(i in 1:100){
    bootstrap_sample <- purrr::map(1:5, function(x){
      df_filtered <- Sensory[which(Sensory$Operator == x), ]
      bootstrap_inds <- sample(1:nrow(df_filtered), bootstrap_size, replace = T)
      return(df_filtered[bootstrap_inds, ])
    }) %>% combine()
    bootstrap_sd[i, ] <- lm(Y ~ Operator, bootstrap_sample) %>% coef()
  }
})
```




## Part c

We can run the bootstraps in parallel since the bootstraps don't depend on one another, i.e. the results of one bootstrap don't affect the results of another.

```{r}
bootstrap_nonparallel_results
```


# Problem 3 (3)

```{r}
Newton <- function(f, fp, x0, tol = 1e-6){
  # Returns a root of the function f with derivative fp at starting point x0 under error tolerance tol
  x <- x0
  x_vals <- c(x)
  f_vals <- c(f(x))
  fp_vals <- c(fp(x))
  diff <- abs(f(x))
  diff_vals <- c(diff)
  # Iterate until y-value at x is below tolerance
  while(diff > tol){
    x <- x - f(x)/fp(x)
    diff <- abs(f(x))
    diff_vals <- c(diff_vals, diff)
    x_vals <- c(x_vals, x)
    f_vals <- c(f_vals, f(x))
    fp_vals <- c(fp_vals, fp(x))
  }
  return(list(
    "root" = x,
    "iterations" = diff_vals,
    "x_vals"     = x_vals,
    "f_vals"     = f_vals,
    "fp_vals"    = fp_vals
  ))
}
```


## Part a

```{r}
f <- function(x){3^x - sin(x) + cos(5*x)}
fp <- function(x){3^x*log(3) - cos(x) - 5*sin(5*x)}
```


```{r}
x_grid <- seq(-10, 10, 1)
```


```{r}
sapply(x_grid, function(x){Newton(f, fp, x)$root})
```


## Part b

```{r}
cl <- makeCluster(8)
clusterExport(cl, "Newton")
clusterExport(cl, "f")
clusterExport(cl, "fp")
parSapply(cl, x_grid, function(x){Newton(f, fp, x)$root})
```


